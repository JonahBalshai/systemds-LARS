 /*
 * Simple demo of LARS optimizer with minimal AlexNet
 */

source("../optim/lars.dml") as lars
source("nn/layers/affine.dml") as affine
source("nn/layers/conv2d_builtin.dml") as conv2d
source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("nn/layers/max_pool2d_builtin.dml") as max_pool2d
source("nn/layers/relu.dml") as relu
source("nn/layers/softmax.dml") as softmax
source("nn/optim/sgd_momentum.dml") as sgd_momentum

print("LARS Demo with Minimal AlexNet")
print("==============================")

# Small data for testing
N = 50    # samples
C = 3     # channels
H = 32    # height (smaller than 224)
W = 32    # width
K = 10    # classes

# Generate dummy data
X = rand(rows=N, cols=C*H*W, min=0, max=1)
y = rand(rows=N, cols=K, min=0, max=0)
for(i in 1:N) {
  class = as.scalar(round(rand(rows=1, cols=1, min=1, max=K)))
  y[i, class] = 1
}

# Mini AlexNet (just 2 conv + 1 fc for demo)
[W1, b1] = conv2d::init(32, C, 5, 5, -1)    # Conv1: 32 5x5 filters
[W2, b2] = conv2d::init(64, 32, 3, 3, -1)   # Conv2: 64 3x3 filters

# Calculate FC input size
# After conv1: 32->32, pool: 32->16
# After conv2: 16->16, pool: 16->8
fc_size = 64 * 8 * 8  # 4096
[W3, b3] = affine::init(fc_size, K, -1)     # FC to output

# Initialize LARS momentum
vW1 = lars::init(W1); vb1 = lars::init(b1)
vW2 = lars::init(W2); vb2 = lars::init(b2)
vW3 = lars::init(W3); vb3 = lars::init(b3)

# Training params
epochs = 1
batch_size = 10
lr = 0.1
mu = 0.9
lambda = 1e-4
trust_coeff = 0.001

print("\nTraining with LARS...")
iters = ceil(N / batch_size)

for (e in 1:epochs) {
  for(i in 1:iters) {
    # Get batch
    beg = ((i-1) * batch_size) %% N + 1
    end = min(N, beg + batch_size - 1)
    X_batch = X[beg:end,]
    y_batch = y[beg:end,]
    
    # Forward pass
    # Conv1 -> ReLU -> Pool
    [outc1, Hc1, Wc1] = conv2d::forward(X_batch, W1, b1, C, H, W, 5, 5, 1, 1, 2, 2)
    outr1 = relu::forward(outc1)
    [outp1, Hp1, Wp1] = max_pool2d::forward(outr1, 32, Hc1, Wc1, 2, 2, 2, 2, 0, 0)
    
    # Conv2 -> ReLU -> Pool
    [outc2, Hc2, Wc2] = conv2d::forward(outp1, W2, b2, 32, Hp1, Wp1, 3, 3, 1, 1, 1, 1)
    outr2 = relu::forward(outc2)
    [outp2, Hp2, Wp2] = max_pool2d::forward(outr2, 64, Hc2, Wc2, 2, 2, 2, 2, 0, 0)
    
    # FC -> Softmax
    outa3 = affine::forward(outp2, W3, b3)
    probs = softmax::forward(outa3)
    
    # Loss
    loss = cross_entropy_loss::forward(probs, y_batch)
    accuracy = mean(rowIndexMax(probs) == rowIndexMax(y_batch))
    
    if (i %% 2 == 0) {
      print("Iter " + i + ": loss = " + loss + ", accuracy = " + accuracy)
    }
    
    # Backward pass
    dprobs = cross_entropy_loss::backward(probs, y_batch)
    douta3 = softmax::backward(dprobs, outa3)
    [doutp2, dW3, db3] = affine::backward(douta3, outp2, W3, b3)
    
    doutr2 = max_pool2d::backward(doutp2, Hp2, Wp2, outr2, 64, Hc2, Wc2, 2, 2, 2, 2, 0, 0)
    doutc2 = relu::backward(doutr2, outc2)
    [doutp1, dW2, db2] = conv2d::backward(doutc2, Hc2, Wc2, outp1, W2, b2, 32, Hp1, Wp1, 3, 3, 1, 1, 1, 1)
    
    doutr1 = max_pool2d::backward(doutp1, Hp1, Wp1, outr1, 32, Hc1, Wc1, 2, 2, 2, 2, 0, 0)
    doutc1 = relu::backward(doutr1, outc1)
    [dX_batch, dW1, db1] = conv2d::backward(doutc1, Hc1, Wc1, X_batch, W1, b1, C, H, W, 5, 5, 1, 1, 2, 2)
    
    # Add L2 regularization
    dW1 = dW1 + lambda * W1
    dW2 = dW2 + lambda * W2
    dW3 = dW3 + lambda * W3
    
    # LARS updates
    [W1, vW1] = lars::update(W1, dW1, lr, mu, vW1, lambda, trust_coeff)
    [b1, vb1] = lars::update(b1, db1, lr, mu, vb1, lambda, trust_coeff)
    [W2, vW2] = lars::update(W2, dW2, lr, mu, vW2, lambda, trust_coeff)
    [b2, vb2] = lars::update(b2, db2, lr, mu, vb2, lambda, trust_coeff)
    [W3, vW3] = lars::update(W3, dW3, lr, mu, vW3, lambda, trust_coeff)
    [b3, vb3] = lars::update(b3, db3, lr, mu, vb3, lambda, trust_coeff)
  }
}

print("\nDemo completed successfully!")
print("LARS optimizer is working with CNN architecture.") 