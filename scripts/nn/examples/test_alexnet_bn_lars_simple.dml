#-------------------------------------------------------------
# Simple test for AlexNet-BN LARS components
#-------------------------------------------------------------

source("nn/networks/alexnet.dml") as alexnet
source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss

print("Testing AlexNet-BN initialization...")

# Test parameters
C = 3
Hin = 224
Win = 224
num_classes = 10
seed = 42

# Test 1: Initialize model with BN
print("Test 1: Initializing AlexNet-BN model...")
[model, emas] = alexnet::init_with_bn(C, Hin, Win, num_classes, seed)
print("Model initialized with " + length(model) + " parameters")
print("EMAs initialized with " + length(emas) + " parameters")

# Test 2: Initialize LARS optimizer
print("\nTest 2: Initializing LARS optimizer state...")
optim_state = alexnet::init_lars_optim_params(model)
print("Optimizer state initialized with " + length(optim_state) + " states")

# Test 3: Test forward pass
print("\nTest 3: Testing forward pass...")
N = 2  # Very small batch
X = rand(rows=N, cols=C*Hin*Win, min=0, max=1, seed=42)
[predictions, cached_out, emas_upd] = alexnet::forward_with_bn(X, C, Hin, Win, model, "train", 0.5)
print("Forward pass completed")
print("Predictions shape: " + nrow(predictions) + " x " + ncol(predictions))

# Test 4: Test loss computation
print("\nTest 4: Testing loss computation...")
Y = table(seq(1, N), sample(num_classes, N, TRUE, 42), N, num_classes)
loss = alexnet::compute_loss(predictions, Y, model, 0.0005)
print("Loss computed: " + loss)

# Test 5: Test learning rate scheduler
print("\nTest 5: Testing learning rate scheduler...")
lr = alexnet::get_lr_with_warmup(0.02, 1, 1, 100, 10, 32, 256, 5, 2)
print("Learning rate: " + lr)

# Test 6: Test LARS hyperparameters
print("\nTest 6: Testing LARS hyperparameter selection...")
[base_lr, warmup_epochs, total_epochs] = alexnet::get_lars_hyperparams(8192, TRUE)
print("Base LR: " + base_lr + ", Warmup: " + warmup_epochs + ", Epochs: " + total_epochs)

print("\nAll component tests passed!")