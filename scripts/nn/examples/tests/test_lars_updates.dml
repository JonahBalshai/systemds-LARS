#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

/*
 * Test script for updated LARS implementation
 * 
 * This script tests:
 * 1. The exact LARS formula from the paper (without weight decay in denominator)
 * 2. The fixed backward pass in AlexNet without dummy gradients
 */

source("nn/optim/lars.dml") as lars
source("nn/networks/alexnet_LARS.dml") as alexnet

test_lars_formula = function() {
  /*
   * Test the LARS optimizer update formula
   */
  print("=== Testing LARS Formula ===")
  
  # Create test parameters and gradients
  X = matrix("1 2 3 4 5 6", rows=2, cols=3)
  dX = matrix("0.1 0.2 0.3 0.4 0.5 0.6", rows=2, cols=3)
  v = lars::init(X)
  
  # Test parameters
  lr = 0.01
  mu = 0.9
  lambda = 0.0001
  trust_coeff = 0.001
  
  print("Initial parameters:")
  print("X = " + toString(X))
  print("dX = " + toString(dX))
  print("||X|| = " + sqrt(sum(X^2)))
  print("||dX|| = " + sqrt(sum(dX^2)))
  
  # Update with LARS
  [X_new, v_new] = lars::update(X, dX, lr, mu, v, lambda, trust_coeff)
  
  print("\nAfter LARS update:")
  print("X_new = " + toString(X_new))
  
  # Verify the computation manually
  X_norm = sqrt(sum(X^2))
  dX_norm = sqrt(sum(dX^2))
  local_lr = trust_coeff * X_norm / (dX_norm + 1e-8)
  effective_lr = lr * local_lr
  
  print("\nManual verification:")
  print("X_norm = " + X_norm)
  print("dX_norm = " + dX_norm)
  print("local_lr = " + local_lr)
  print("effective_lr = " + effective_lr)
  
  # Test with small parameters (should use global lr)
  X_small = matrix("0.0001 0.0002", rows=1, cols=2)
  dX_small = matrix("0.1 0.2", rows=1, cols=2)
  v_small = lars::init(X_small)
  
  print("\n\nTesting with small parameters (bias-like):")
  print("X_small = " + toString(X_small))
  print("||X_small|| = " + sqrt(sum(X_small^2)))
  
  [X_small_new, v_small_new] = lars::update(X_small, dX_small, lr, mu, v_small, lambda, trust_coeff)
  print("X_small_new = " + toString(X_small_new))
  
  print("\n✅ LARS formula test completed!")
}

test_alexnet_backward = function() {
  /*
   * Test AlexNet backward pass without dummy gradients
   */
  print("\n\n=== Testing AlexNet Backward Pass ===")
  
  # Small test parameters
  N = 2
  C = 3
  Hin = 224
  Win = 224
  num_classes = 10
  
  # Create test data
  X = rand(rows=N, cols=C*Hin*Win, min=0, max=1, seed=42)
  Y = table(seq(1, N), sample(num_classes, N, TRUE, 42), N, num_classes)
  
  # Initialize model with BN
  [model, emas] = alexnet::init_with_bn(C, Hin, Win, num_classes, 42)
  
  print("Model initialized with " + length(model) + " parameters")
  
  # Forward pass
  [predictions, cached_out, emas_upd] = alexnet::forward_with_bn(
      X, C, Hin, Win, model, "train", 0.5)
  
  print("Forward pass completed")
  print("Predictions shape: " + nrow(predictions) + "x" + ncol(predictions))
  
  # Compute loss gradient
  # For cross-entropy loss, gradient is (predictions - targets) / N
  dOut = (predictions - Y) / N
  
  print("Loss gradient computed")
  
  # Backward pass
  start_time = time()
  [dX, gradients] = alexnet::backward_with_bn(dOut, cached_out, model, C, Hin, Win, 0.5)
  backward_time = (time() - start_time) / 1000.0
  
  print("Backward pass completed in " + backward_time + " seconds")
  print("Number of gradients: " + length(gradients))
  
  # Verify gradients are reasonable
  grad_norms = matrix(0, rows=length(gradients), cols=1)
  for (i in 1:length(gradients)) {
    grad = as.matrix(gradients[i])
    grad_norm = sqrt(sum(grad^2))
    grad_norms[i] = grad_norm
  }
  
  print("\nGradient norms (first 10):")
  for (i in 1:min(10, length(gradients))) {
    print("  Gradient " + i + ": " + as.scalar(grad_norms[i]))
  }
  
  # Check if any gradients are zero (which would indicate a problem)
  # Note: EMA parameters (exponential moving averages) for batch norm should have zero gradients
  zero_grads = sum(grad_norms == 0)
  if (zero_grads > 0) {
    print("Note: " + zero_grads + " gradients are zero (expected for EMA parameters in BN)")
    # Count how many are exactly at indices 5,6,11,12,17,18,23,24,29,30 (EMA positions)
    ema_positions = list(5, 6, 11, 12, 17, 18, 23, 24, 29, 30)
    expected_zeros = 0
    for (i in 1:length(ema_positions)) {
      pos = as.scalar(ema_positions[i])
      if (as.scalar(grad_norms[pos]) == 0) {
        expected_zeros = expected_zeros + 1
      }
    }
    if (expected_zeros == zero_grads) {
      print("✅ All zero gradients are for EMA parameters as expected")
    } else {
      print("WARNING: Some unexpected zero gradients found!")
    }
  } else {
    print("✅ All gradients are non-zero")
  }
  
  print("\n✅ AlexNet backward pass test completed!")
}

test_lars_integration = function() {
  /*
   * Test LARS integration with AlexNet
   */
  print("\n\n=== Testing LARS Integration with AlexNet ===")
  
  # Small test
  N = 2
  C = 3
  Hin = 224
  Win = 224
  num_classes = 10
  batch_size = 2
  
  # Create test data
  X = rand(rows=N, cols=C*Hin*Win, min=0, max=1, seed=42)
  Y = table(seq(1, N), sample(num_classes, N, TRUE, 42), N, num_classes)
  
  # Initialize model
  [model, emas] = alexnet::init_with_bn(C, Hin, Win, num_classes, 42)
  optim_state = alexnet::init_lars_optim_params(model)
  
  print("Model and optimizer initialized")
  
  # Training parameters
  lr = 0.01
  momentum = 0.9
  weight_decay = 0.0005
  trust_coeff = 0.001
  
  # Run one training iteration
  print("\nRunning one training iteration...")
  
  # Forward pass
  [predictions, cached_out, emas_upd] = alexnet::forward_with_bn(
      X, C, Hin, Win, model, "train", 0.5)
  
  # Compute loss
  loss = alexnet::compute_loss(predictions, Y, model, weight_decay)
  acc = alexnet::compute_accuracy(predictions, Y)
  print("Initial loss: " + loss + ", accuracy: " + acc)
  
  # Backward pass
  dOut = (predictions - Y) / N
  [dX, gradients] = alexnet::backward_with_bn(dOut, cached_out, model, C, Hin, Win, 0.5)
  
  # Update with LARS
  [model_upd, optim_state_upd] = alexnet::update_params_with_lars(
      model, gradients, lr, momentum, weight_decay, trust_coeff, optim_state)
  
  # Forward pass with updated model
  [predictions_upd, cached_out_upd, emas_upd2] = alexnet::forward_with_bn(
      X, C, Hin, Win, model_upd, "train", 0.5)
  
  # Compute updated loss
  loss_upd = alexnet::compute_loss(predictions_upd, Y, model_upd, weight_decay)
  acc_upd = alexnet::compute_accuracy(predictions_upd, Y)
  print("Updated loss: " + loss_upd + ", accuracy: " + acc_upd)
  
  # Check if loss decreased (not guaranteed for one iteration, but good sign)
  if (loss_upd < loss) {
    print("✅ Loss decreased after update")
  } else {
    print("⚠️  Loss increased after update (can happen in early training)")
  }
  
  print("\n✅ LARS integration test completed!")
}

# Run all tests
print("Starting LARS implementation tests...\n")

test_lars_formula()
test_alexnet_backward()
test_lars_integration()

print("\n\n=== All tests completed successfully! ===")