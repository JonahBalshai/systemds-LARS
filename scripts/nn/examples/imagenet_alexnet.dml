#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

/*
 * ImageNet AlexNet Example with support for different optimizers
 * 
 * This implements the full AlexNet architecture for ImageNet (224x224 images)
 * with support for testing different optimizers including LARS
 */

# Imports
source("nn/layers/affine.dml") as affine
source("nn/layers/conv2d_builtin.dml") as conv2d
source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("nn/layers/dropout.dml") as dropout
source("nn/layers/l2_reg.dml") as l2_reg
source("nn/layers/max_pool2d_builtin.dml") as max_pool2d
source("nn/layers/relu.dml") as relu
source("nn/layers/softmax.dml") as softmax
source("nn/layers/batch_norm2d.dml") as batch_norm2d
source("../layers/lrn.dml") as lrn

# Import optimizers
source("nn/optim/sgd.dml") as sgd
source("nn/optim/sgd_momentum.dml") as sgd_momentum
source("nn/optim/sgd_nesterov.dml") as sgd_nesterov
source("nn/optim/adam.dml") as adam
source("nn/optim/adamw.dml") as adamw
source("nn/optim/adagrad.dml") as adagrad
source("nn/optim/rmsprop.dml") as rmsprop

# Import LARS optimizer
source("../optim/lars.dml") as lars

train = function(matrix[double] X, matrix[double] Y,
                 matrix[double] X_val, matrix[double] Y_val,
                 int C, int Hin, int Win, int epochs,
                 string optimizer, double lr, int batch_size)
    return (list[unknown] model) {
  /*
   * Trains AlexNet architecture with specified optimizer.
   *
   * Original AlexNet architecture for ImageNet:
   * - Conv1: 96 filters of 11x11, stride 4, pad 0
   * - MaxPool1: 3x3, stride 2
   * - Conv2: 256 filters of 5x5, stride 1, pad 2  
   * - MaxPool2: 3x3, stride 2
   * - Conv3: 384 filters of 3x3, stride 1, pad 1
   * - Conv4: 384 filters of 3x3, stride 1, pad 1
   * - Conv5: 256 filters of 3x3, stride 1, pad 1
   * - MaxPool3: 3x3, stride 2
   * - FC6: 4096 neurons
   * - FC7: 4096 neurons
   * - FC8: 1000 neurons (ImageNet classes)
   *
   * Inputs:
   *  - X: Input data matrix, of shape (N, C*Hin*Win).
   *  - Y: Target matrix, of shape (N, K).
   *  - X_val: Input validation data matrix, of shape (N, C*Hin*Win).
   *  - Y_val: Target validation matrix, of shape (N, K).
   *  - C: Number of input channels (3 for RGB).
   *  - Hin: Input height (224 for ImageNet).
   *  - Win: Input width (224 for ImageNet).
   *  - epochs: Number of epochs.
   *  - optimizer: Optimizer name ("sgd", "adam", "lars", etc.).
   *  - lr: Learning rate.
   *  - batch_size: Batch size.
   *
   * Outputs:
   *  - model: List containing all model parameters.
   */
  N = nrow(X)
  K = ncol(Y)

  # Initialize network parameters
  # Conv layers - Original AlexNet sizes
  [W1, b1] = conv2d::init(96, C, 11, 11, -1)      # Conv1: 96 11x11 filters
  [W2, b2] = conv2d::init(256, 96, 5, 5, -1)      # Conv2: 256 5x5 filters  
  [W3, b3] = conv2d::init(384, 256, 3, 3, -1)     # Conv3: 384 3x3 filters
  [W4, b4] = conv2d::init(384, 384, 3, 3, -1)     # Conv4: 384 3x3 filters
  [W5, b5] = conv2d::init(256, 384, 3, 3, -1)     # Conv5: 256 3x3 filters

  # Calculate size after convolutions and pooling for ImageNet (224x224)
  # After conv1: (224-11)/4+1 = 54, after pool1: (54-3)/2+1 = 26
  # After conv2: (26+2*2-5)/1+1 = 26, after pool2: (26-3)/2+1 = 12
  # After conv3,4,5: 12->12->12->12, after pool3: (12-3)/2+1 = 5
  # Final output: 5x5 feature maps with 256 channels
  fc_input_size = 256 * 5 * 5  # 6400

  # Fully connected layers
  [W6, b6] = affine::init(fc_input_size, 4096, -1)  # FC6
  [W7, b7] = affine::init(4096, 4096, -1)           # FC7
  [W8, b8] = affine::init(4096, K, -1)              # FC8 (output)
  
  # Different initialization for final layer
  W8 = W8 / sqrt(2)

  # Initialize batch normalization parameters for conv layers
  [gamma1, beta1, ema_mean1, ema_var1] = batch_norm2d::init(96)
  [gamma2, beta2, ema_mean2, ema_var2] = batch_norm2d::init(256)
  [gamma3, beta3, ema_mean3, ema_var3] = batch_norm2d::init(384)
  [gamma4, beta4, ema_mean4, ema_var4] = batch_norm2d::init(384)
  [gamma5, beta5, ema_mean5, ema_var5] = batch_norm2d::init(256)

  # Initialize optimizer states based on optimizer type
  if (optimizer == "sgd") {
    # No additional state needed for vanilla SGD
    opt_state = list()
  }
  else if (optimizer == "sgd_momentum" | optimizer == "sgd_nesterov") {
    vW1 = sgd_momentum::init(W1); vb1 = sgd_momentum::init(b1)
    vW2 = sgd_momentum::init(W2); vb2 = sgd_momentum::init(b2)
    vW3 = sgd_momentum::init(W3); vb3 = sgd_momentum::init(b3)
    vW4 = sgd_momentum::init(W4); vb4 = sgd_momentum::init(b4)
    vW5 = sgd_momentum::init(W5); vb5 = sgd_momentum::init(b5)
    vW6 = sgd_momentum::init(W6); vb6 = sgd_momentum::init(b6)
    vW7 = sgd_momentum::init(W7); vb7 = sgd_momentum::init(b7)
    vW8 = sgd_momentum::init(W8); vb8 = sgd_momentum::init(b8)
    # BN parameters
    vgamma1 = sgd_momentum::init(gamma1); vbeta1 = sgd_momentum::init(beta1)
    vgamma2 = sgd_momentum::init(gamma2); vbeta2 = sgd_momentum::init(beta2)
    vgamma3 = sgd_momentum::init(gamma3); vbeta3 = sgd_momentum::init(beta3)
    vgamma4 = sgd_momentum::init(gamma4); vbeta4 = sgd_momentum::init(beta4)
    vgamma5 = sgd_momentum::init(gamma5); vbeta5 = sgd_momentum::init(beta5)
  }
  else if (optimizer == "adam" | optimizer == "adamw") {
    [mW1, vW1] = adam::init(W1); [mb1, vb1] = adam::init(b1)
    [mW2, vW2] = adam::init(W2); [mb2, vb2] = adam::init(b2)
    [mW3, vW3] = adam::init(W3); [mb3, vb3] = adam::init(b3)
    [mW4, vW4] = adam::init(W4); [mb4, vb4] = adam::init(b4)
    [mW5, vW5] = adam::init(W5); [mb5, vb5] = adam::init(b5)
    [mW6, vW6] = adam::init(W6); [mb6, vb6] = adam::init(b6)
    [mW7, vW7] = adam::init(W7); [mb7, vb7] = adam::init(b7)
    [mW8, vW8] = adam::init(W8); [mb8, vb8] = adam::init(b8)
    t = 1
  }
  else if (optimizer == "lars") {
    # LARS uses momentum internally
    vW1 = lars::init(W1); vb1 = lars::init(b1)
    vW2 = lars::init(W2); vb2 = lars::init(b2)
    vW3 = lars::init(W3); vb3 = lars::init(b3)
    vW4 = lars::init(W4); vb4 = lars::init(b4)
    vW5 = lars::init(W5); vb5 = lars::init(b5)
    vW6 = lars::init(W6); vb6 = lars::init(b6)
    vW7 = lars::init(W7); vb7 = lars::init(b7)
    vW8 = lars::init(W8); vb8 = lars::init(b8)
    # BN parameters
    vgamma1 = lars::init(gamma1); vbeta1 = lars::init(beta1)
    vgamma2 = lars::init(gamma2); vbeta2 = lars::init(beta2)
    vgamma3 = lars::init(gamma3); vbeta3 = lars::init(beta3)
    vgamma4 = lars::init(gamma4); vbeta4 = lars::init(beta4)
    vgamma5 = lars::init(gamma5); vbeta5 = lars::init(beta5)
  }

  # Training parameters
  lambda = 5e-4  # L2 regularization
  mu = 0.9       # Momentum
  beta1 = 0.9    # Adam beta1
  beta2 = 0.999  # Adam beta2
  epsilon = 1e-8 # Adam epsilon
  decay = 0.95   # Learning rate decay
  
  # Training loop
  print("Starting AlexNet training with " + optimizer + " optimizer")
  print("Learning rate: " + lr + ", Batch size: " + batch_size)
  
  iters = ceil(N / batch_size)
  for (e in 1:epochs) {
    for(i in 1:iters) {
      # Get next batch
      beg = ((i-1) * batch_size) %% N + 1
      end = min(N, beg + batch_size - 1)
      X_batch = X[beg:end,]
      y_batch = Y[beg:end,]

      # Forward pass
      # Conv1 -> BN -> ReLU -> Pool1 -> LRN
      [outc1, Houtc1, Woutc1] = conv2d::forward(X_batch, W1, b1, C, Hin, Win, 11, 11, 4, 4, 0, 0)
      [outbn1, cache_mean1, cache_var1, cache_norm1] = batch_norm2d::forward(outc1, gamma1, beta1, 96, Houtc1, Woutc1, "train", ema_mean1, ema_var1, 0.99, 1e-5)
      outr1 = relu::forward(outbn1)
      [outp1, Houtp1, Woutp1] = max_pool2d::forward(outr1, 96, Houtc1, Woutc1, 3, 3, 2, 2, 0, 0)
      outlrn1 = lrn::forward(outp1, 96, Houtp1, Woutp1, 5, 0.0001, 0.75, 2)
      
      # Conv2 -> BN -> ReLU -> Pool2 -> LRN
      [outc2, Houtc2, Woutc2] = conv2d::forward(outlrn1, W2, b2, 96, Houtp1, Woutp1, 5, 5, 1, 1, 2, 2)
      [outbn2, cache_mean2, cache_var2, cache_norm2] = batch_norm2d::forward(outc2, gamma2, beta2, 256, Houtc2, Woutc2, "train", ema_mean2, ema_var2, 0.99, 1e-5)
      outr2 = relu::forward(outbn2)
      [outp2, Houtp2, Woutp2] = max_pool2d::forward(outr2, 256, Houtc2, Woutc2, 3, 3, 2, 2, 0, 0)
      outlrn2 = lrn::forward(outp2, 256, Houtp2, Woutp2, 5, 0.0001, 0.75, 2)
      
      # Conv3 -> BN -> ReLU
      [outc3, Houtc3, Woutc3] = conv2d::forward(outlrn2, W3, b3, 256, Houtp2, Woutp2, 3, 3, 1, 1, 1, 1)
      [outbn3, cache_mean3, cache_var3, cache_norm3] = batch_norm2d::forward(outc3, gamma3, beta3, 384, Houtc3, Woutc3, "train", ema_mean3, ema_var3, 0.99, 1e-5)
      outr3 = relu::forward(outbn3)
      
      # Conv4 -> BN -> ReLU
      [outc4, Houtc4, Woutc4] = conv2d::forward(outr3, W4, b4, 384, Houtc3, Woutc3, 3, 3, 1, 1, 1, 1)
      [outbn4, cache_mean4, cache_var4, cache_norm4] = batch_norm2d::forward(outc4, gamma4, beta4, 384, Houtc4, Woutc4, "train", ema_mean4, ema_var4, 0.99, 1e-5)
      outr4 = relu::forward(outbn4)
      
      # Conv5 -> BN -> ReLU -> Pool3
      [outc5, Houtc5, Woutc5] = conv2d::forward(outr4, W5, b5, 384, Houtc4, Woutc4, 3, 3, 1, 1, 1, 1)
      [outbn5, cache_mean5, cache_var5, cache_norm5] = batch_norm2d::forward(outc5, gamma5, beta5, 256, Houtc5, Woutc5, "train", ema_mean5, ema_var5, 0.99, 1e-5)
      outr5 = relu::forward(outbn5)
      [outp5, Houtp5, Woutp5] = max_pool2d::forward(outr5, 256, Houtc5, Woutc5, 3, 3, 2, 2, 0, 0)
      
      # FC6 -> ReLU -> Dropout
      outa6 = affine::forward(outp5, W6, b6)
      outr6 = relu::forward(outa6)
      [outd6, maskd6] = dropout::forward(outr6, 0.5, -1)
      
      # FC7 -> ReLU -> Dropout
      outa7 = affine::forward(outd6, W7, b7)
      outr7 = relu::forward(outa7)
      [outd7, maskd7] = dropout::forward(outr7, 0.5, -1)
      
      # FC8 -> Softmax
      outa8 = affine::forward(outd7, W8, b8)
      probs = softmax::forward(outa8)

      # Compute loss and accuracy every 100 iterations
      if (i %% 100 == 0) {
        loss_data = cross_entropy_loss::forward(probs, y_batch)
        loss_reg = lambda * (l2_reg::forward(W1, 1) + l2_reg::forward(W2, 1) + 
                            l2_reg::forward(W3, 1) + l2_reg::forward(W4, 1) + 
                            l2_reg::forward(W5, 1) + l2_reg::forward(W6, 1) + 
                            l2_reg::forward(W7, 1) + l2_reg::forward(W8, 1))
        loss = loss_data + loss_reg
        
        # Top-1 and Top-5 accuracy
        top1_accuracy = mean(rowIndexMax(probs) == rowIndexMax(y_batch))
        
        # Top-5 accuracy calculation
        sorted_probs = order(target=probs, by=1, decreasing=TRUE, index.return=FALSE)
        top5_preds = sorted_probs[,1:5]
        true_labels = rowIndexMax(y_batch)
        top5_correct = matrix(0, rows=nrow(probs), cols=1)
        for (j in 1:nrow(probs)) {
          for (k in 1:5) {
            if (as.scalar(top5_preds[j,k]) == as.scalar(true_labels[j,1])) {
              top5_correct[j,1] = 1
            }
          }
        }
        top5_accuracy = mean(top5_correct)

        print("Epoch: " + e + ", Iter: " + i + ", Train Loss: " + loss + 
              ", Top-1 Acc: " + top1_accuracy + ", Top-5 Acc: " + top5_accuracy)
      }

      # Backward pass
      dprobs = cross_entropy_loss::backward(probs, y_batch)
      
      # FC8
      douta8 = softmax::backward(dprobs, outa8)
      [doutd7, dW8, db8] = affine::backward(douta8, outr7, W8, b8)
      
      # FC7
      doutr7 = dropout::backward(doutd7, outr7, 0.5, maskd7)
      douta7 = relu::backward(doutr7, outa7)
      [doutd6, dW7, db7] = affine::backward(douta7, outr6, W7, b7)
      
      # FC6
      doutr6 = dropout::backward(doutd6, outr6, 0.5, maskd6)
      douta6 = relu::backward(doutr6, outa6)
      [doutp5, dW6, db6] = affine::backward(douta6, outp5, W6, b6)
      
      # Conv5
      doutr5 = max_pool2d::backward(doutp5, Houtp5, Woutp5, outr5, 256, Houtc5, Woutc5, 3, 3, 2, 2, 0, 0)
      doutbn5 = relu::backward(doutr5, outbn5)
      [doutc5, dgamma5, dbeta5] = batch_norm2d::backward(doutbn5, cache_mean5, cache_var5, cache_norm5, gamma5, 256, Houtc5, Woutc5, 1e-5)
      [doutr4, dW5, db5] = conv2d::backward(doutc5, Houtc5, Woutc5, outr4, W5, b5, 384, Houtc4, Woutc4, 3, 3, 1, 1, 1, 1)
      
      # Conv4
      doutbn4 = relu::backward(doutr4, outbn4)
      [doutc4, dgamma4, dbeta4] = batch_norm2d::backward(doutbn4, cache_mean4, cache_var4, cache_norm4, gamma4, 384, Houtc4, Woutc4, 1e-5)
      [doutr3, dW4, db4] = conv2d::backward(doutc4, Houtc4, Woutc4, outr3, W4, b4, 384, Houtc3, Woutc3, 3, 3, 1, 1, 1, 1)
      
      # Conv3
      doutbn3 = relu::backward(doutr3, outbn3)
      [doutc3, dgamma3, dbeta3] = batch_norm2d::backward(doutbn3, cache_mean3, cache_var3, cache_norm3, gamma3, 384, Houtc3, Woutc3, 1e-5)
      [doutlrn2, dW3, db3] = conv2d::backward(doutc3, Houtc3, Woutc3, outlrn2, W3, b3, 256, Houtp2, Woutp2, 3, 3, 1, 1, 1, 1)
      
      # Conv2
      doutp2 = lrn::backward(doutlrn2, outp2, 256, Houtp2, Woutp2, 5, 0.0001, 0.75, 2)
      doutr2 = max_pool2d::backward(doutp2, Houtp2, Woutp2, outr2, 256, Houtc2, Woutc2, 3, 3, 2, 2, 0, 0)
      doutbn2 = relu::backward(doutr2, outbn2)
      [doutc2, dgamma2, dbeta2] = batch_norm2d::backward(doutbn2, cache_mean2, cache_var2, cache_norm2, gamma2, 256, Houtc2, Woutc2, 1e-5)
      [doutlrn1, dW2, db2] = conv2d::backward(doutc2, Houtc2, Woutc2, outlrn1, W2, b2, 96, Houtp1, Woutp1, 5, 5, 1, 1, 2, 2)
      
      # Conv1
      doutp1 = lrn::backward(doutlrn1, outp1, 96, Houtp1, Woutp1, 5, 0.0001, 0.75, 2)
      doutr1 = max_pool2d::backward(doutp1, Houtp1, Woutp1, outr1, 96, Houtc1, Woutc1, 3, 3, 2, 2, 0, 0)
      doutbn1 = relu::backward(doutr1, outbn1)
      [doutc1, dgamma1, dbeta1] = batch_norm2d::backward(doutbn1, cache_mean1, cache_var1, cache_norm1, gamma1, 96, Houtc1, Woutc1, 1e-5)
      [dX_batch, dW1, db1] = conv2d::backward(doutc1, Houtc1, Woutc1, X_batch, W1, b1, C, Hin, Win, 11, 11, 4, 4, 0, 0)

      # Add L2 regularization gradients
      dW1 = dW1 + lambda * l2_reg::backward(W1, 1)
      dW2 = dW2 + lambda * l2_reg::backward(W2, 1)
      dW3 = dW3 + lambda * l2_reg::backward(W3, 1)
      dW4 = dW4 + lambda * l2_reg::backward(W4, 1)
      dW5 = dW5 + lambda * l2_reg::backward(W5, 1)
      dW6 = dW6 + lambda * l2_reg::backward(W6, 1)
      dW7 = dW7 + lambda * l2_reg::backward(W7, 1)
      dW8 = dW8 + lambda * l2_reg::backward(W8, 1)

      # Update parameters based on optimizer
      if (optimizer == "sgd") {
        W1 = sgd::update(W1, dW1, lr); b1 = sgd::update(b1, db1, lr)
        W2 = sgd::update(W2, dW2, lr); b2 = sgd::update(b2, db2, lr)
        W3 = sgd::update(W3, dW3, lr); b3 = sgd::update(b3, db3, lr)
        W4 = sgd::update(W4, dW4, lr); b4 = sgd::update(b4, db4, lr)
        W5 = sgd::update(W5, dW5, lr); b5 = sgd::update(b5, db5, lr)
        W6 = sgd::update(W6, dW6, lr); b6 = sgd::update(b6, db6, lr)
        W7 = sgd::update(W7, dW7, lr); b7 = sgd::update(b7, db7, lr)
        W8 = sgd::update(W8, dW8, lr); b8 = sgd::update(b8, db8, lr)
        gamma1 = sgd::update(gamma1, dgamma1, lr); beta1 = sgd::update(beta1, dbeta1, lr)
        gamma2 = sgd::update(gamma2, dgamma2, lr); beta2 = sgd::update(beta2, dbeta2, lr)
        gamma3 = sgd::update(gamma3, dgamma3, lr); beta3 = sgd::update(beta3, dbeta3, lr)
        gamma4 = sgd::update(gamma4, dgamma4, lr); beta4 = sgd::update(beta4, dbeta4, lr)
        gamma5 = sgd::update(gamma5, dgamma5, lr); beta5 = sgd::update(beta5, dbeta5, lr)
      }
      else if (optimizer == "sgd_momentum") {
        [W1, vW1] = sgd_momentum::update(W1, dW1, lr, mu, vW1); [b1, vb1] = sgd_momentum::update(b1, db1, lr, mu, vb1)
        [W2, vW2] = sgd_momentum::update(W2, dW2, lr, mu, vW2); [b2, vb2] = sgd_momentum::update(b2, db2, lr, mu, vb2)
        [W3, vW3] = sgd_momentum::update(W3, dW3, lr, mu, vW3); [b3, vb3] = sgd_momentum::update(b3, db3, lr, mu, vb3)
        [W4, vW4] = sgd_momentum::update(W4, dW4, lr, mu, vW4); [b4, vb4] = sgd_momentum::update(b4, db4, lr, mu, vb4)
        [W5, vW5] = sgd_momentum::update(W5, dW5, lr, mu, vW5); [b5, vb5] = sgd_momentum::update(b5, db5, lr, mu, vb5)
        [W6, vW6] = sgd_momentum::update(W6, dW6, lr, mu, vW6); [b6, vb6] = sgd_momentum::update(b6, db6, lr, mu, vb6)
        [W7, vW7] = sgd_momentum::update(W7, dW7, lr, mu, vW7); [b7, vb7] = sgd_momentum::update(b7, db7, lr, mu, vb7)
        [W8, vW8] = sgd_momentum::update(W8, dW8, lr, mu, vW8); [b8, vb8] = sgd_momentum::update(b8, db8, lr, mu, vb8)
        [gamma1, vgamma1] = sgd_momentum::update(gamma1, dgamma1, lr, mu, vgamma1); [beta1, vbeta1] = sgd_momentum::update(beta1, dbeta1, lr, mu, vbeta1)
        [gamma2, vgamma2] = sgd_momentum::update(gamma2, dgamma2, lr, mu, vgamma2); [beta2, vbeta2] = sgd_momentum::update(beta2, dbeta2, lr, mu, vbeta2)
        [gamma3, vgamma3] = sgd_momentum::update(gamma3, dgamma3, lr, mu, vgamma3); [beta3, vbeta3] = sgd_momentum::update(beta3, dbeta3, lr, mu, vbeta3)
        [gamma4, vgamma4] = sgd_momentum::update(gamma4, dgamma4, lr, mu, vgamma4); [beta4, vbeta4] = sgd_momentum::update(beta4, dbeta4, lr, mu, vbeta4)
        [gamma5, vgamma5] = sgd_momentum::update(gamma5, dgamma5, lr, mu, vgamma5); [beta5, vbeta5] = sgd_momentum::update(beta5, dbeta5, lr, mu, vbeta5)
      }
      else if (optimizer == "lars") {
        # LARS updates with layer-wise adaptive learning rates
        trust_coeff = 0.001  # LARS trust coefficient
        [W1, vW1] = lars::update(W1, dW1, lr, mu, vW1, lambda, trust_coeff)
        [b1, vb1] = lars::update(b1, db1, lr, mu, vb1, lambda, trust_coeff)
        [W2, vW2] = lars::update(W2, dW2, lr, mu, vW2, lambda, trust_coeff)
        [b2, vb2] = lars::update(b2, db2, lr, mu, vb2, lambda, trust_coeff)
        [W3, vW3] = lars::update(W3, dW3, lr, mu, vW3, lambda, trust_coeff)
        [b3, vb3] = lars::update(b3, db3, lr, mu, vb3, lambda, trust_coeff)
        [W4, vW4] = lars::update(W4, dW4, lr, mu, vW4, lambda, trust_coeff)
        [b4, vb4] = lars::update(b4, db4, lr, mu, vb4, lambda, trust_coeff)
        [W5, vW5] = lars::update(W5, dW5, lr, mu, vW5, lambda, trust_coeff)
        [b5, vb5] = lars::update(b5, db5, lr, mu, vb5, lambda, trust_coeff)
        [W6, vW6] = lars::update(W6, dW6, lr, mu, vW6, lambda, trust_coeff)
        [b6, vb6] = lars::update(b6, db6, lr, mu, vb6, lambda, trust_coeff)
        [W7, vW7] = lars::update(W7, dW7, lr, mu, vW7, lambda, trust_coeff)
        [b7, vb7] = lars::update(b7, db7, lr, mu, vb7, lambda, trust_coeff)
        [W8, vW8] = lars::update(W8, dW8, lr, mu, vW8, lambda, trust_coeff)
        [b8, vb8] = lars::update(b8, db8, lr, mu, vb8, lambda, trust_coeff)
        [gamma1, vgamma1] = lars::update(gamma1, dgamma1, lr, mu, vgamma1, 0, trust_coeff)
        [beta1, vbeta1] = lars::update(beta1, dbeta1, lr, mu, vbeta1, 0, trust_coeff)
        [gamma2, vgamma2] = lars::update(gamma2, dgamma2, lr, mu, vgamma2, 0, trust_coeff)
        [beta2, vbeta2] = lars::update(beta2, dbeta2, lr, mu, vbeta2, 0, trust_coeff)
        [gamma3, vgamma3] = lars::update(gamma3, dgamma3, lr, mu, vgamma3, 0, trust_coeff)
        [beta3, vbeta3] = lars::update(beta3, dbeta3, lr, mu, vbeta3, 0, trust_coeff)
        [gamma4, vgamma4] = lars::update(gamma4, dgamma4, lr, mu, vgamma4, 0, trust_coeff)
        [beta4, vbeta4] = lars::update(beta4, dbeta4, lr, mu, vbeta4, 0, trust_coeff)
        [gamma5, vgamma5] = lars::update(gamma5, dgamma5, lr, mu, vgamma5, 0, trust_coeff)
        [beta5, vbeta5] = lars::update(beta5, dbeta5, lr, mu, vbeta5, 0, trust_coeff)
      }
    }
    
    # Learning rate decay
    lr = lr * decay
  }

  # Package model
  model = list(W1=W1, b1=b1, W2=W2, b2=b2, W3=W3, b3=b3, W4=W4, b4=b4, 
               W5=W5, b5=b5, W6=W6, b6=b6, W7=W7, b7=b7, W8=W8, b8=b8,
               gamma1=gamma1, beta1=beta1, ema_mean1=ema_mean1, ema_var1=ema_var1,
               gamma2=gamma2, beta2=beta2, ema_mean2=ema_mean2, ema_var2=ema_var2,
               gamma3=gamma3, beta3=beta3, ema_mean3=ema_mean3, ema_var3=ema_var3,
               gamma4=gamma4, beta4=beta4, ema_mean4=ema_mean4, ema_var4=ema_var4,
               gamma5=gamma5, beta5=beta5, ema_mean5=ema_mean5, ema_var5=ema_var5)
}

predict = function(matrix[double] X, int C, int Hin, int Win,
                   matrix[double] W1, matrix[double] b1,
                   matrix[double] W2, matrix[double] b2,
                   matrix[double] W3, matrix[double] b3,
                   matrix[double] W4, matrix[double] b4,
                   matrix[double] W5, matrix[double] b5,
                   matrix[double] W6, matrix[double] b6,
                   matrix[double] W7, matrix[double] b7,
                   matrix[double] W8, matrix[double] b8,
                   matrix[double] gamma1, matrix[double] beta1,
                   matrix[double] ema_mean1, matrix[double] ema_var1,
                   matrix[double] gamma2, matrix[double] beta2,
                   matrix[double] ema_mean2, matrix[double] ema_var2,
                   matrix[double] gamma3, matrix[double] beta3,
                   matrix[double] ema_mean3, matrix[double] ema_var3,
                   matrix[double] gamma4, matrix[double] beta4,
                   matrix[double] ema_mean4, matrix[double] ema_var4,
                   matrix[double] gamma5, matrix[double] beta5,
                   matrix[double] ema_mean5, matrix[double] ema_var5)
    return (matrix[double] probs) {
  /*
   * Computes predictions for AlexNet.
   */
  N = nrow(X)
  K = ncol(W8)
  
  # Compute predictions over mini-batches
  probs = matrix(0, rows=N, cols=K)
  batch_size = 64
  iters = ceil(N / batch_size)
  
  for(i in 1:iters) {
    # Get next batch
    beg = ((i-1) * batch_size) %% N + 1
    end = min(N, beg + batch_size - 1)
    X_batch = X[beg:end,]

    # Forward pass (test mode - no dropout)
    # Conv1 -> BN -> ReLU -> Pool1 -> LRN
    [outc1, Houtc1, Woutc1] = conv2d::forward(X_batch, W1, b1, C, Hin, Win, 11, 11, 4, 4, 0, 0)
    [outbn1, cache_mean1, cache_var1, cache_norm1] = batch_norm2d::forward(outc1, gamma1, beta1, 96, Houtc1, Woutc1, "test", ema_mean1, ema_var1, 0.99, 1e-5)
    outr1 = relu::forward(outbn1)
    [outp1, Houtp1, Woutp1] = max_pool2d::forward(outr1, 96, Houtc1, Woutc1, 3, 3, 2, 2, 0, 0)
    outlrn1 = lrn::forward(outp1, 96, Houtp1, Woutp1, 5, 0.0001, 0.75, 2)
    
    # Conv2 -> BN -> ReLU -> Pool2 -> LRN
    [outc2, Houtc2, Woutc2] = conv2d::forward(outlrn1, W2, b2, 96, Houtp1, Woutp1, 5, 5, 1, 1, 2, 2)
    [outbn2, cache_mean2, cache_var2, cache_norm2] = batch_norm2d::forward(outc2, gamma2, beta2, 256, Houtc2, Woutc2, "test", ema_mean2, ema_var2, 0.99, 1e-5)
    outr2 = relu::forward(outbn2)
    [outp2, Houtp2, Woutp2] = max_pool2d::forward(outr2, 256, Houtc2, Woutc2, 3, 3, 2, 2, 0, 0)
    outlrn2 = lrn::forward(outp2, 256, Houtp2, Woutp2, 5, 0.0001, 0.75, 2)
    
    # Conv3 -> BN -> ReLU
    [outc3, Houtc3, Woutc3] = conv2d::forward(outlrn2, W3, b3, 256, Houtp2, Woutp2, 3, 3, 1, 1, 1, 1)
    [outbn3, cache_mean3, cache_var3, cache_norm3] = batch_norm2d::forward(outc3, gamma3, beta3, 384, Houtc3, Woutc3, "test", ema_mean3, ema_var3, 0.99, 1e-5)
    outr3 = relu::forward(outbn3)
    
    # Conv4 -> BN -> ReLU
    [outc4, Houtc4, Woutc4] = conv2d::forward(outr3, W4, b4, 384, Houtc3, Woutc3, 3, 3, 1, 1, 1, 1)
    [outbn4, cache_mean4, cache_var4, cache_norm4] = batch_norm2d::forward(outc4, gamma4, beta4, 384, Houtc4, Woutc4, "test", ema_mean4, ema_var4, 0.99, 1e-5)
    outr4 = relu::forward(outbn4)
    
    # Conv5 -> BN -> ReLU -> Pool3
    [outc5, Houtc5, Woutc5] = conv2d::forward(outr4, W5, b5, 384, Houtc4, Woutc4, 3, 3, 1, 1, 1, 1)
    [outbn5, cache_mean5, cache_var5, cache_norm5] = batch_norm2d::forward(outc5, gamma5, beta5, 256, Houtc5, Woutc5, "test", ema_mean5, ema_var5, 0.99, 1e-5)
    outr5 = relu::forward(outbn5)
    [outp5, Houtp5, Woutp5] = max_pool2d::forward(outr5, 256, Houtc5, Woutc5, 3, 3, 2, 2, 0, 0)
    
    # FC6 -> ReLU (no dropout in test mode)
    outa6 = affine::forward(outp5, W6, b6)
    outr6 = relu::forward(outa6)
    
    # FC7 -> ReLU (no dropout in test mode)
    outa7 = affine::forward(outr6, W7, b7)
    outr7 = relu::forward(outa7)
    
    # FC8 -> Softmax
    outa8 = affine::forward(outr7, W8, b8)
    probs_batch = softmax::forward(outa8)
    
    # Store predictions
    probs[beg:end,] = probs_batch
  }
}

eval = function(matrix[double] probs, matrix[double] Y)
    return (double loss, double top1_accuracy, double top5_accuracy) {
  /*
   * Evaluates predictions with top-1 and top-5 accuracy.
   */
  loss = cross_entropy_loss::forward(probs, Y)
  
  # Top-1 accuracy
  correct_pred = rowIndexMax(probs) == rowIndexMax(Y)
  top1_accuracy = mean(correct_pred)
  
  # Top-5 accuracy
  sorted_probs = order(target=probs, by=1, decreasing=TRUE, index.return=FALSE)
  top5_preds = sorted_probs[,1:5]
  true_labels = rowIndexMax(Y)
  top5_correct = matrix(0, rows=nrow(probs), cols=1)
  for (j in 1:nrow(probs)) {
    for (k in 1:5) {
      if (as.scalar(top5_preds[j,k]) == as.scalar(true_labels[j,1])) {
        top5_correct[j,1] = 1
      }
    }
  }
  top5_accuracy = mean(top5_correct)
} 