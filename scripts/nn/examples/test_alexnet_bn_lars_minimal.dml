#-------------------------------------------------------------
# Minimal test for AlexNet-BN LARS training loop
#-------------------------------------------------------------

source("nn/networks/alexnet.dml") as alexnet
source("nn/util.dml") as util
source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("nn/layers/l2_reg.dml") as l2_reg

print("Testing minimal AlexNet-BN LARS training...")

# Test parameters
C = 3
Hin = 224  # Standard AlexNet input
Win = 224  # Standard AlexNet input
num_classes = 10
batch_size = 4
epochs = 1
base_lr = 0.02
seed = 42

# Create small dataset
N_train = 8
N_val = 4
D = C * Hin * Win

X_train = rand(rows=N_train, cols=D, min=0, max=1, seed=42)
Y_train = table(seq(1, N_train), sample(num_classes, N_train, TRUE, 42), N_train, num_classes)
X_val = rand(rows=N_val, cols=D, min=0, max=1, seed=43)
Y_val = table(seq(1, N_val), sample(num_classes, N_val, TRUE, 43), N_val, num_classes)

print("Data created: Train=" + N_train + " samples, Val=" + N_val + " samples")

# Initialize model with BN
[model, emas] = alexnet::init_with_bn(C, Hin, Win, num_classes, seed)
print("Model initialized with " + length(model) + " parameters")

# Initialize LARS optimizer
optim_state = alexnet::init_lars_optim_params(model)
print("Optimizer initialized")

# LARS parameters
momentum = 0.9
weight_decay = 0.0005
trust_coeff = 0.001
base_batch_size = 256
warmup_epochs = 1
decay_power = 2

# Training metrics
train_losses = matrix(0, rows=epochs, cols=1)
val_accs = matrix(0, rows=epochs, cols=1)

# Calculate iterations per epoch
iters_per_epoch = ceil(N_train / batch_size)
print("Iterations per epoch: " + iters_per_epoch)

# Training loop
for (epoch in 1:epochs) {
  print("\nEpoch " + epoch)
  epoch_loss = 0
  
  for (iter in 1:iters_per_epoch) {
    # Get learning rate
    lr = alexnet::get_lr_with_warmup(base_lr, epoch, iter, epochs, 
                                     iters_per_epoch, batch_size, 
                                     base_batch_size, warmup_epochs, decay_power)
    
    # Get batch
    beg = ((iter-1) * batch_size) %% N_train + 1
    end = min(N_train, beg + batch_size - 1)
    X_batch = X_train[beg:end,]
    Y_batch = Y_train[beg:end,]
    
    print("  Iter " + iter + ", batch " + beg + ":" + end + ", LR=" + lr)
    
    # Forward pass
    [predictions, cached_out, emas_upd] = alexnet::forward_with_bn(
        X_batch, C, Hin, Win, model, "train", 0.5)
    
    # Update EMAs (simplified - just copy them back)
    model[5] = as.matrix(emas_upd[1])
    model[6] = as.matrix(emas_upd[2])
    model[11] = as.matrix(emas_upd[3])
    model[12] = as.matrix(emas_upd[4])
    model[17] = as.matrix(emas_upd[5])
    model[18] = as.matrix(emas_upd[6])
    model[23] = as.matrix(emas_upd[7])
    model[24] = as.matrix(emas_upd[8])
    model[29] = as.matrix(emas_upd[9])
    model[30] = as.matrix(emas_upd[10])
    
    # Compute loss
    batch_loss = alexnet::compute_loss(predictions, Y_batch, model, weight_decay)
    epoch_loss = epoch_loss + batch_loss
    print("    Loss: " + batch_loss)
    
    # For testing, skip backward pass and just use dummy gradients
    gradients = list()
    for (i in 1:length(model)) {
      param = as.matrix(model[i])
      grad = rand(rows=nrow(param), cols=ncol(param), min=-0.01, max=0.01, seed=i)
      gradients = append(gradients, grad)
    }
    
    # Update with LARS
    [model, optim_state] = alexnet::update_params_with_lars(
        model, gradients, lr, momentum, weight_decay, trust_coeff, optim_state)
  }
  
  # Epoch metrics
  train_losses[epoch,1] = epoch_loss / iters_per_epoch
  avg_loss = as.scalar(train_losses[epoch,1])
  print("  Average epoch loss: " + avg_loss)
  
  # Simple validation
  [val_predictions, val_cached, val_emas] = alexnet::forward_with_bn(
      X_val, C, Hin, Win, model, "test", 0.0)
  val_loss = alexnet::compute_loss(val_predictions, Y_val, model, 0.0)
  val_acc = alexnet::compute_accuracy(val_predictions, Y_val)
  val_accs[epoch,1] = val_acc
  
  print("  Validation - Loss: " + val_loss + ", Acc: " + val_acc)
}

print("\nTest completed successfully!")
final_loss = as.scalar(train_losses[epochs,1])
final_acc = as.scalar(val_accs[epochs,1])
print("Final train loss: " + final_loss)
print("Final val acc: " + final_acc)